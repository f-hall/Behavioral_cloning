{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I left the old comments, from the first submit in the code and added some, where they are neede\n",
    "# with RESUBMIT: beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "from keras.layers import ELU\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import AveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import all images and steering angles from data.\n",
    "# I used the 'left', 'center' and 'right' images from the provided udacity dataset\n",
    "# Choosing steering angles y_left = y_center + 0.3 and y_right = y_center - 0.3 to get more data\n",
    "# From the provided data I take only approx. 20% of the center images to get less data with steering angle equal to zero.\n",
    "# Furthermore I used data that I produced with the simulator (that has only center images) for the more\n",
    "# difficult curves on the track.\n",
    "\n",
    "# In addition I flipped all images and associated steering angles to get more data.\n",
    "# Resizing the pictures to 64x32 pix, normalizing and cutting the upper part of the pictures,\n",
    "# so I get approx. 52.000 64x22 pix images.\n",
    "# Did not need a python generator or gpu, which is nice.\n",
    "\n",
    "# Shuffling x_array / y_array\n",
    "\n",
    "#...............................................................................................\n",
    "\n",
    "# RESUBMIT: Used a python generator instead of loading all data in memory. The training of the model\n",
    "# is now double the time before, because you have to read from disk all the time. But it is advantages\n",
    "# when you don't have enough memory to store all image data in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_file = \"/media/frank/Zusatz/CarND-P3/data_add/driving_log.csv\"\n",
    "with open (training_file) as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    x_array = []\n",
    "    y_array = []\n",
    "    for row in reader:\n",
    "        _, x = row['center'].split('IMG/')\n",
    "        y = float(row['steering'])\n",
    "        yl = y + 0.3\n",
    "        yr = y - 0.3\n",
    "        space = 0\n",
    "        if not row['left'].isspace():\n",
    "            _, l = row['left'].split('IMG/')\n",
    "            _, r = row['right'].split('IMG/')\n",
    "            space = 1\n",
    "        rnd = np.random.uniform()\n",
    "        if (rnd > 0.8) or (space == 0):\n",
    "            x_array.append(x)\n",
    "            y_array.append(y)\n",
    "        if (space == 1):\n",
    "            x_array.append(l)\n",
    "            x_array.append(r)\n",
    "            y_array.append(yl)\n",
    "            y_array.append(yr)\n",
    "            \n",
    "x_array = np.asarray(x_array)\n",
    "y_array = np.asarray(y_array)\n",
    "x_array, y_array = shuffle(x_array, y_array)\n",
    "x_train, x_val = np.split(x_array, [int(0.8*len(x_array))])\n",
    "y_train, y_val = np.split(y_array, [int(0.8*len(y_array))])\n",
    "print(x_array.shape)\n",
    "print(y_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen(x, y, batch_size):\n",
    "    rows = x.shape[0]\n",
    "    counter = None\n",
    "    while 1:\n",
    "        X_list = []\n",
    "        Y_list = []\n",
    "        for i in range(batch_size):\n",
    "            if counter is None or counter >= rows:\n",
    "                counter = 0\n",
    "            X = x[counter]\n",
    "            Y = y[counter]\n",
    "            X = Image.open('/media/frank/Zusatz/CarND-P3/data_add/IMG/'+X)\n",
    "            X = np.asarray(X)\n",
    "            X = cv2.resize(X, None, fx=0.2, fy=0.2)\n",
    "            X = X / 255\n",
    "            X = X[10:32, 0:64]\n",
    "            flip_rnd = np.random.uniform()\n",
    "            if flip_rnd > 0.5:\n",
    "                X = cv2.flip(X, 1)\n",
    "                Y = -1 * Y\n",
    "            X_list.append(X)\n",
    "            Y_list.append(Y)\n",
    "            counter = counter + 1\n",
    "        X_list = np.asarray(X_list).reshape(-1, 22, 64, 3)\n",
    "        Y_list = np.asarray(Y_list)\n",
    "        yield X_list, Y_list\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Definition of my model used for learing, testing and using it in the simulation.\n",
    "# It was just a ton of trial and error to get to this, but lastly I think that the\n",
    "# chosen data and the preprocessing is much more important.\n",
    "\n",
    "# model: Conv (8x2x2xsame) - AvgPool(2x2) - Dropout(0.25) - ELU activation\n",
    "# - Conv(16x2x2xsame) - ELU activation - Conv(32x2x2xsame) - ELU activation \n",
    "# - Conv(48x2x2xsame) - ELU activation - Flatten - Dense(256) - Dropout(0.40)\n",
    "# - ELU activation - Dense(128) - ELU activation - Dense(64) - Dropout(0.40)\n",
    "# - ELU activation - Dense(32) - ELU activation - Dense(1)\n",
    "\n",
    "# I have choosen mse and rmsprop. rmsprop seems to work better for me than \n",
    "# adam optimizer for different learning rates does. \n",
    "# Default lr (0.001)\n",
    "\n",
    "#..............................................................................\n",
    "\n",
    "# RESUBMIT: Used BatchNormalization before each ELU-Activationlayer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(8, 2, 2, input_shape=(22, 64, 3)))\n",
    "    model.add(AveragePooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ELU())\n",
    "    model.add(Convolution2D(16, 2, 2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ELU())\n",
    "    model.add(Convolution2D(32, 2, 2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ELU())\n",
    "    model.add(Convolution2D(48, 2, 2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ELU())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dropout(0.40))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ELU())\n",
    "    model.add(Dense(128))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ELU())\n",
    "    model.add(Dense(64))\n",
    "    model.add(Dropout(0.40))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ELU())\n",
    "    model.add(Dense(32))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ELU())\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    model.compile(loss = 'mse', optimizer='rmsprop')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Testing showed batch_size 256 is good for me.\n",
    "# Validationsplit 20% of data to prevent overfitting.\n",
    "# Used 15 epochs before, but 30 seems to be more stable and with\n",
    "# the dropout it seems I'm far away from overfitting, so it should not be a problem.\n",
    "# Maybe 25 or 20 epoch are enough, but I did not test this.\n",
    "# For me it was exactly 1 min for each epoch, so 30 minutes for the whole thing.\n",
    "\n",
    "#.............................................................................................\n",
    "\n",
    "# RESUBMIT: Used model.fit_generator instead of model.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "model.fit_generator(gen(x_train, y_train, 256), samples_per_epoch=x_train[0], nb_epoch=30, validation_data=gen(x_val, y_val, 256), nb_val_samples=x_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving the model and the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I used more testing cells in this notebook before but cutted them out before the\n",
    "# last version for a better general view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:CarND-LeNet-Lab]",
   "language": "python",
   "name": "conda-env-CarND-LeNet-Lab-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
